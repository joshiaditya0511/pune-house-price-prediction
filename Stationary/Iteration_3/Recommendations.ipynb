{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Null values:  localityName           0\n",
      "price                  0\n",
      "carpetArea          3764\n",
      "floorNumber            0\n",
      "totalFloorNumber       0\n",
      "transactionType        0\n",
      "furnished             37\n",
      "bedrooms               0\n",
      "bathrooms              0\n",
      "ageofcons           2571\n",
      "dtype: int64 \n",
      "\n",
      "Test Set Null values:  localityName          0\n",
      "price                 0\n",
      "carpetArea          977\n",
      "floorNumber           0\n",
      "totalFloorNumber      0\n",
      "transactionType       0\n",
      "furnished            14\n",
      "bedrooms              0\n",
      "bathrooms             0\n",
      "ageofcons           693\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "\n",
    "dtype_mapping = {\n",
    "    'propertyId': pd.StringDtype(),\n",
    "    'localityName': 'category',\n",
    "    'landMarks': pd.StringDtype(),\n",
    "    'locality': pd.StringDtype(),\n",
    "    'price': pd.Int64Dtype(),\n",
    "    'nameOfSociety': pd.StringDtype(),\n",
    "    'projectName': pd.StringDtype(),\n",
    "    'carpetArea': pd.Int64Dtype(),\n",
    "    'coveredArea': pd.Int64Dtype(),\n",
    "    'carpetAreaSqft': pd.Int64Dtype(),\n",
    "    'possessionStatus': pd.StringDtype(),\n",
    "    'developerName': pd.StringDtype(),\n",
    "    'flooringType': pd.StringDtype(),\n",
    "    'floorNumber': pd.Int64Dtype(),\n",
    "    'unitCountonFloor': pd.Int64Dtype(),\n",
    "    'totalFloorNumber': pd.Int64Dtype(),\n",
    "    'electricityStatus': pd.StringDtype(),\n",
    "    'waterStatus': pd.StringDtype(),\n",
    "    'longitude': pd.Float64Dtype(),\n",
    "    'latitude': pd.Float64Dtype(),\n",
    "    'transactionType': 'category',\n",
    "    'facing': pd.StringDtype(),\n",
    "    'ownershipType': pd.StringDtype(),\n",
    "    'carParking': pd.StringDtype(),\n",
    "    'furnished': 'category',\n",
    "    'bedrooms': pd.Int64Dtype(),\n",
    "    'bathrooms': pd.Int64Dtype(),\n",
    "    'numberOfBalconied': pd.Int64Dtype(),\n",
    "    'propertyType': 'category',\n",
    "    'additionalRooms': pd.StringDtype(),\n",
    "    'bookingAmountExact': pd.Int64Dtype(),\n",
    "    'maintenanceChargesFrequency': 'category',\n",
    "    'maintenanceCharges': pd.Int64Dtype(),\n",
    "    'ageofcons': 'category',\n",
    "    'isVerified': 'category',\n",
    "    'listingTypeDesc': 'category',\n",
    "    'premiumProperty': pd.BooleanDtype(),\n",
    "    'noOfLifts': pd.Int64Dtype(),\n",
    "    'propertyAmenities': pd.StringDtype(),\n",
    "    'facilitiesDesc': pd.StringDtype(),\n",
    "    'uuid': pd.StringDtype(),\n",
    "    'flooringType_Vitrified': pd.BooleanDtype(),\n",
    "    'flooringType_CeramicTiles': pd.BooleanDtype(),\n",
    "    'flooringType_Marble': pd.BooleanDtype(),\n",
    "    'flooringType_NormalTilesKotahStone': pd.BooleanDtype(),\n",
    "    'flooringType_Granite': pd.BooleanDtype(),\n",
    "    'flooringType_Wooden': pd.BooleanDtype(),\n",
    "    'flooringType_Mosaic': pd.BooleanDtype(),\n",
    "    'flooringType_Marbonite': pd.BooleanDtype(),\n",
    "    'additionalRoom_PujaRoom': pd.BooleanDtype(),\n",
    "    'additionalRoom_Study': pd.BooleanDtype(),\n",
    "    'additionalRoom_Store': pd.BooleanDtype(),\n",
    "    'additionalRoom_ServantRoom': pd.BooleanDtype(),\n",
    "    'carParking_Open': pd.Int64Dtype(),\n",
    "    'carParking_Covered': pd.Int64Dtype(),\n",
    "    'ReservedParking': pd.BooleanDtype(),\n",
    "}\n",
    "\n",
    "COLUMNS_TO_DROP = [\n",
    "    'coveredArea',\n",
    "    'ReservedParking',\n",
    "] + [\n",
    "        'unitCountonFloor',\n",
    "        'electricityStatus',\n",
    "        'waterStatus',\n",
    "        'facing',\n",
    "        'bookingAmountExact',\n",
    "        'isVerified',\n",
    "        'listingTypeDesc',\n",
    "        'maintenanceCharges',\n",
    "        'maintenanceChargesFrequency',\n",
    "        'latitude',\n",
    "        'longitude',\n",
    "        'carParking_Open',\n",
    "        'carParking_Covered',\n",
    "        'numberOfBalconied',\n",
    "        'premiumProperty',\n",
    "        'projectName',\n",
    "        'nameOfSociety',\n",
    "        'url',\n",
    "        # 'uuid',\n",
    "        'carpetAreaSqft',\n",
    "        'noOfLifts',\n",
    "        'ownershipType',\n",
    "        'possessionStatus',\n",
    "        'propertyType',\n",
    "\n",
    "        'flooringType_Vitrified',\n",
    "        'flooringType_CeramicTiles',\n",
    "        'flooringType_Marble',\n",
    "        'flooringType_NormalTilesKotahStone',\n",
    "        'flooringType_Granite',\n",
    "        'flooringType_Wooden',\n",
    "        'flooringType_Mosaic',\n",
    "        'flooringType_Marbonite',\n",
    "\n",
    "        'additionalRoom_PujaRoom',\n",
    "        'additionalRoom_Study',\n",
    "        'additionalRoom_Store',\n",
    "        'additionalRoom_ServantRoom',\n",
    "        \n",
    "        'landMarks', \n",
    "        'locality', \n",
    "        'developerName',]\n",
    "\n",
    "################################################################################\n",
    "# ONLY USING THE RAW SETs, NOT IMPUTED SET\n",
    "################################################################################\n",
    "df_train = pd.read_csv(\n",
    "    'Data/train.csv',\n",
    "    dtype = dtype_mapping,\n",
    "    index_col=0\n",
    ")\n",
    "df_train.drop(COLUMNS_TO_DROP, axis=1, inplace=True)\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    'Data/test.csv',\n",
    "    dtype = dtype_mapping,\n",
    "    index_col=0\n",
    ")\n",
    "df_test.drop(COLUMNS_TO_DROP, axis=1, inplace=True)\n",
    "\n",
    "################################################################################\n",
    "# DROPPING ALL ROWS WITH MISSING VALUES\n",
    "################################################################################\n",
    "\n",
    "print(\"Train Set Null values: \", df_train.isna().sum(), '\\n')\n",
    "print(\"Test Set Null values: \", df_test.isna().sum(), '\\n')\n",
    "\n",
    "df_train.dropna(axis=0, inplace=True)\n",
    "df_test.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"carpetArea\": pd.Int64Dtype(),\n",
    "    \"floorNumber\": pd.Int64Dtype(),\n",
    "    \"totalFloorNumber\": pd.Int64Dtype(),\n",
    "    \"bedrooms\": pd.Int64Dtype(),\n",
    "    \"bathrooms\": pd.Int64Dtype(),\n",
    "    \"localityName\": 'category',\n",
    "    \"transactionType\": 'category',\n",
    "    \"furnished\": 'category',\n",
    "    \"ageofcons\": 'category',\n",
    "}\n",
    "\n",
    "df = pd.concat([df_train, df_test], ignore_index=False).astype(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# DROPPING LOCALITY FOR NEAREST NEIGHBORS\n",
    "########################################################################\n",
    "\n",
    "# df.drop(columns=['localityName'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17452 entries, 65067453 to 77784815\n",
      "Data columns (total 10 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   localityName      17452 non-null  category\n",
      " 1   price             17452 non-null  Int64   \n",
      " 2   carpetArea        17452 non-null  Int64   \n",
      " 3   floorNumber       17452 non-null  Int64   \n",
      " 4   totalFloorNumber  17452 non-null  Int64   \n",
      " 5   transactionType   17452 non-null  category\n",
      " 6   furnished         17452 non-null  category\n",
      " 7   bedrooms          17452 non-null  Int64   \n",
      " 8   bathrooms         17452 non-null  Int64   \n",
      " 9   ageofcons         17452 non-null  category\n",
      "dtypes: Int64(6), category(4)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and transforming data...\n",
      "Data transformed. Shape: (17452, 5)\n",
      "Applying feature weights...\n",
      "Reordering columns...\n",
      "\n",
      "--- Final Encoded Vectors ---\n",
      "Shape: (17452, 5)\n",
      "Columns: ['carpetArea', 'bedrooms', 'bathrooms', 'floorNumber', 'totalFloorNumber']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np # For numerical operations if needed later\n",
    "\n",
    "# 1. Define feature lists based on type\n",
    "numerical_cols = [\n",
    "    \"carpetArea\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"floorNumber\",\n",
    "    \"totalFloorNumber\",\n",
    "]\n",
    "categorical_cols = [\n",
    "    # \"localityName\",\n",
    "    # \"transactionType\",\n",
    "    # \"furnished\",\n",
    "    # \"ageofcons\",\n",
    "]\n",
    "\n",
    "# 2. Define the desired final order (matching user input expectation)\n",
    "# Note: Categorical features will be expanded by OneHotEncoder\n",
    "user_input_order = [\n",
    "    \"carpetArea\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"floorNumber\",\n",
    "    \"totalFloorNumber\",\n",
    "    # \"localityName\",\n",
    "    # \"transactionType\",\n",
    "    # \"furnished\",\n",
    "    # \"ageofcons\",\n",
    "]\n",
    "\n",
    "# 3. Define weights (currently all 1.0)\n",
    "# We'll apply these *after* initial scaling/encoding\n",
    "feature_weights = {\n",
    "    \"carpetArea\": 1.3,\n",
    "    \"bedrooms\": 1.1,\n",
    "    \"bathrooms\": 1.0,\n",
    "    \"floorNumber\": 1.0,\n",
    "    \"totalFloorNumber\": 1.125,\n",
    "    # \"localityName\": 1.0, # Weight applies to all generated OHE columns\n",
    "    # \"transactionType\": 1.0, # Weight applies to all generated OHE columns\n",
    "    # \"furnished\": 1.0, # Weight applies to all generated OHE columns\n",
    "    # \"ageofcons\": 1.0, # Weight applies to all generated OHE columns\n",
    "}\n",
    "\n",
    "# --- Preprocessing ---\n",
    "\n",
    "# Separate features (X) and property IDs (index)\n",
    "# Exclude 'price' column\n",
    "features_to_encode = numerical_cols + categorical_cols\n",
    "X = df[features_to_encode]\n",
    "property_ids = df.index # Preserve property IDs\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "# - Numerical features: Standard Scaling\n",
    "# - Categorical features: One-Hot Encoding\n",
    "#   - handle_unknown='ignore': If user input has a category not seen in training,\n",
    "#     it will be encoded as all zeros for that feature. Important for robustness.\n",
    "#   - sparse_output=False: Output a dense numpy array, easier to work with.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            numerical_cols,\n",
    "        ),\n",
    "        # (\n",
    "        #     \"cat\",\n",
    "        #     OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "        #     categorical_cols,\n",
    "        # ),\n",
    "    ],\n",
    "    remainder=\"passthrough\", # Keep other columns if any (shouldn't be any here)\n",
    "    verbose_feature_names_out=False, # Keep original names for num features\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the data and transform it\n",
    "print(\"Fitting and transforming data...\")\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "print(f\"Data transformed. Shape: {X_encoded.shape}\")\n",
    "\n",
    "# Get the feature names after transformation (important for OHE)\n",
    "# This preserves numerical names and creates names like 'localityName_XYZ'\n",
    "encoded_feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame with the encoded data and correct column names\n",
    "encoded_df = pd.DataFrame(\n",
    "    X_encoded, columns=encoded_feature_names, index=property_ids\n",
    ")\n",
    "\n",
    "# --- Apply Weighting ---\n",
    "print(\"Applying feature weights...\")\n",
    "weighted_df = encoded_df.copy() # Start with the encoded data\n",
    "\n",
    "for feature_name, weight in feature_weights.items():\n",
    "    if weight == 1.0: # No need to multiply if weight is 1\n",
    "        continue\n",
    "\n",
    "    # Find columns corresponding to this original feature\n",
    "    # For numerical, it's just the name.\n",
    "    # For categorical, it's the name + '_' + category value\n",
    "    if feature_name in numerical_cols:\n",
    "        cols_to_weight = [feature_name]\n",
    "    elif feature_name in categorical_cols:\n",
    "        # Find all columns starting with the categorical feature name + \"_\"\n",
    "        # (default separator) or exactly the feature name if OHE created it differently\n",
    "        # (less likely with verbose_feature_names_out=False)\n",
    "        cols_to_weight = [\n",
    "            col\n",
    "            for col in encoded_feature_names\n",
    "            if col.startswith(feature_name + \"_\") or col == feature_name\n",
    "        ]\n",
    "    else:\n",
    "        cols_to_weight = [] # Should not happen with current setup\n",
    "\n",
    "    if not cols_to_weight:\n",
    "        print(f\"Warning: No columns found for weighting feature '{feature_name}'\")\n",
    "        continue\n",
    "\n",
    "    # Apply the weight by multiplying the selected columns\n",
    "    weighted_df[cols_to_weight] *= weight\n",
    "\n",
    "# --- Reorder Columns to Match User Input Structure ---\n",
    "# We need to reconstruct the final order, expanding the categorical names\n",
    "print(\"Reordering columns...\")\n",
    "final_column_order = []\n",
    "current_encoded_cols = weighted_df.columns.tolist()\n",
    "\n",
    "for feature in user_input_order:\n",
    "    if feature in numerical_cols:\n",
    "        final_column_order.append(feature)\n",
    "    elif feature in categorical_cols:\n",
    "        # Find all columns that were generated from this categorical feature\n",
    "        generated_cols = [\n",
    "            col\n",
    "            for col in current_encoded_cols\n",
    "            if col.startswith(feature + \"_\") or col == feature # Handle potential edge cases\n",
    "        ]\n",
    "        # Sort them alphabetically for consistency (optional but good practice)\n",
    "        generated_cols.sort()\n",
    "        final_column_order.extend(generated_cols)\n",
    "\n",
    "# Create the final DataFrame with the desired column order\n",
    "final_encoded_vectors = weighted_df[final_column_order]\n",
    "\n",
    "# --- Output ---\n",
    "print(\"\\n--- Final Encoded Vectors ---\")\n",
    "print(f\"Shape: {final_encoded_vectors.shape}\")\n",
    "print(\"Columns:\", final_encoded_vectors.columns.tolist())\n",
    "\n",
    "\n",
    "# --- Important for Backend ---\n",
    "# You will need to SAVE:\n",
    "# 1. The `final_encoded_vectors` DataFrame (or its numpy array version).\n",
    "#    This contains the vectors you'll search against.\n",
    "#    Example: final_encoded_vectors.to_pickle(\"property_vectors.pkl\")\n",
    "#             or np.save(\"property_vectors.npy\", final_encoded_vectors.values)\n",
    "#             and save property_ids separately if using numpy array.\n",
    "#\n",
    "# 2. The fitted `preprocessor` object. You NEED this to encode the user's input\n",
    "#    in the exact same way before performing the nearest neighbor search.\n",
    "#    Example: import joblib\n",
    "#             joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "#\n",
    "# 3. The `final_column_order` list (or derive it again in the backend) if needed\n",
    "#    for verification, although the preprocessor handles the transformation order.\n",
    "#\n",
    "# 4. The `property_ids` (which are the index of `final_encoded_vectors`).\n",
    "#    You need these to map the indices returned by NearestNeighbors back to actual IDs.\n",
    "#    Example: (already saved if using pickle for the DataFrame)\n",
    "#             or save separately: pd.Series(property_ids).to_pickle(\"property_ids.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2595"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_encoded_vectors.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_encoded_vectors.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17452 entries, 65067453 to 77784815\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   carpetArea        17452 non-null  float64\n",
      " 1   bedrooms          17452 non-null  float64\n",
      " 2   bathrooms         17452 non-null  float64\n",
      " 3   floorNumber       17452 non-null  float64\n",
      " 4   totalFloorNumber  17452 non-null  float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 818.1 KB\n"
     ]
    }
   ],
   "source": [
    "final_encoded_vectors.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting NearestNeighbors model (k=10, metric='cosine')...\n",
      "NearestNeighbors model fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import joblib # For saving the model later\n",
    "\n",
    "# --- Assume 'property_ids' exists (which is the index of final_encoded_vectors) ---\n",
    "property_ids = final_encoded_vectors.index\n",
    "\n",
    "# --- Configuration ---\n",
    "N_NEIGHBORS = 10 # How many neighbors to find by default\n",
    "METRIC = 'cosine' # Distance metric ('cosine', 'euclidean'/'l2', 'manhattan'/'l1')\n",
    "\n",
    "# --- Fit Nearest Neighbors Model ---\n",
    "\n",
    "print(f\"Fitting NearestNeighbors model (k={N_NEIGHBORS}, metric='{METRIC}')...\")\n",
    "\n",
    "# 1. Initialize the model\n",
    "#    n_jobs=-1 uses all available CPU cores for potentially faster fitting/querying\n",
    "nn_model = NearestNeighbors(\n",
    "    n_neighbors=N_NEIGHBORS, metric=METRIC, algorithm='auto', n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2. Fit the model on the encoded data vectors\n",
    "#    It's generally recommended to pass the underlying NumPy array (.values)\n",
    "nn_model.fit(final_encoded_vectors.values)\n",
    "\n",
    "print(\"NearestNeighbors model fitted successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Trial Run: Find neighbors for a sample property ---\n",
    "\n",
    "# # 1. Choose a sample property to find neighbors for (e.g., the first one)\n",
    "# sample_index_position = 0 # Taking the first property in the DataFrame\n",
    "# sample_property_id = property_ids[sample_index_position]\n",
    "# sample_vector = final_encoded_vectors.iloc[[sample_index_position]] # Keep as DataFrame row initially\n",
    "\n",
    "# print(f\"\\n--- Finding neighbors for sample property ID: {sample_property_id} ---\")\n",
    "# print(f\"Sample Vector (head): \\n{sample_vector.iloc[:, :5]}\") # Show first few features\n",
    "\n",
    "# # 2. Prepare the sample vector for kneighbors (needs to be 2D NumPy array)\n",
    "# sample_vector_np = sample_vector.values # Get NumPy array (already 2D)\n",
    "\n",
    "# # 3. Use the fitted model to find neighbors\n",
    "# #    kneighbors returns distances and indices\n",
    "# distances, indices = nn_model.kneighbors(sample_vector_np)\n",
    "\n",
    "# print(f\"\\nRaw distances: {distances}\")\n",
    "# print(f\"Raw indices: {indices}\")\n",
    "\n",
    "# # The `indices` array contains the row positions (0-based) in the\n",
    "# # original `final_encoded_vectors` data that are the nearest neighbors.\n",
    "# # Since we queried with one sample, indices[0] contains the list of neighbor indices.\n",
    "\n",
    "# # 4. Extract the indices for our single sample query\n",
    "# neighbor_indices = indices[0]\n",
    "# print(f\"\\nIndices of the {N_NEIGHBORS} nearest neighbors: {neighbor_indices}\")\n",
    "\n",
    "# # 5. Map these indices back to the actual Property IDs\n",
    "# #    We use the original `property_ids` Series/Index we stored earlier\n",
    "# retrieved_neighbor_ids = property_ids[neighbor_indices].tolist()\n",
    "\n",
    "# print(f\"\\nRetrieved Property IDs of neighbors: {retrieved_neighbor_ids}\")\n",
    "\n",
    "# # --- Verification (Optional) ---\n",
    "# # The first neighbor (index 0) should usually be the sample property itself,\n",
    "# # unless there's an exact duplicate vector elsewhere.\n",
    "# if retrieved_neighbor_ids[0] == sample_property_id:\n",
    "#     print(\"\\nVerification: The first neighbor is the sample property itself (expected).\")\n",
    "# else:\n",
    "#     print(\"\\nVerification: The first neighbor is NOT the sample property itself.\")\n",
    "\n",
    "\n",
    "# # --- Important for Backend ---\n",
    "# # Now you need to SAVE the fitted `nn_model` object.\n",
    "# # Example:\n",
    "# # joblib.dump(nn_model, 'nearest_neighbors_model.joblib')\n",
    "# #\n",
    "# # Remember you also need the saved `preprocessor` and the `property_ids`\n",
    "# # (or the `final_encoded_vectors` DataFrame which includes the IDs as index)\n",
    "# # from the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Processing + Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np # Only needed if you were saving numpy arrays directly\n",
    "\n",
    "# It's good practice to define filenames as constants\n",
    "NN_MODEL_FILE = 'PipelinesAndModels/RecommendationsPython/nearest_neighbors_model_iteration_3.joblib'\n",
    "RECOMMENDATION_PREPROCESSOR_FILE = 'PipelinesAndModels/RecommendationsPython/recommendation_preprocessor_iteration_3.joblib'\n",
    "PROPERTY_VECTORS_FILE = 'PipelinesAndModels/RecommendationsPython/property_vectors_iteration_3.pkl' # Using pickle for the DataFrame\n",
    "\n",
    "joblib.dump(nn_model, NN_MODEL_FILE, compress=3)\n",
    "\n",
    "joblib.dump(preprocessor, RECOMMENDATION_PREPROCESSOR_FILE, compress=3)\n",
    "\n",
    "final_encoded_vectors.to_pickle(PROPERTY_VECTORS_FILE)\n",
    "\n",
    "# --- Optional: Save Property IDs separately (if needed, but redundant if saving the DataFrame) ---\n",
    "# property_ids = final_encoded_vectors.index\n",
    "# PROPERTY_IDS_FILE = 'property_ids.pkl'\n",
    "# print(f\"Saving property IDs separately to: {PROPERTY_IDS_FILE}\")\n",
    "# pd.Series(property_ids).to_pickle(PROPERTY_IDS_FILE)\n",
    "# print(\"-> Saved property_ids.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling property Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving All the recommendations metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhagy\\AppData\\Local\\Temp\\ipykernel_49076\\2626004421.py:3: DtypeWarning: Columns (34,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rawData = pd.read_csv(\"Data/rawExtractedPropertyDetails.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rawData = pd.read_csv(\"Data/rawExtractedPropertyDetails.csv\")\n",
    "rawData['propertyId'] = rawData['propertyId'].astype(str)\n",
    "rawData.set_index('propertyId', inplace=True)\n",
    "\n",
    "metadata_features = [\n",
    "    \"carpetArea\",\n",
    "    \"bedrooms\",\n",
    "    \"bathrooms\",\n",
    "    \"floorNumber\",\n",
    "    \"totalFloorNumber\",\n",
    "    \"localityName\",\n",
    "    \"transactionType\",\n",
    "    \"furnished\",\n",
    "    \"ageofcons\",\n",
    "    'localityName',\n",
    "    'price'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'carpetArea',\n",
    "    'bedrooms',\n",
    "    'bathrooms',\n",
    "    'floorNumber',\n",
    "    'totalFloorNumber',\n",
    "    'price'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'localityName',\n",
    "    'transactionType',\n",
    "    'furnished',\n",
    "    'ageofcons'\n",
    "]\n",
    "\n",
    "df_metadata = df[numerical_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17452 entries, 65067453 to 77784815\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   carpetArea        17452 non-null  Int64\n",
      " 1   bedrooms          17452 non-null  Int64\n",
      " 2   bathrooms         17452 non-null  Int64\n",
      " 3   floorNumber       17452 non-null  Int64\n",
      " 4   totalFloorNumber  17452 non-null  Int64\n",
      " 5   price             17452 non-null  Int64\n",
      "dtypes: Int64(6)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17452 entries, 65067453 to 77784815\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   carpetArea        17452 non-null  Int64\n",
      " 1   bedrooms          17452 non-null  Int64\n",
      " 2   bathrooms         17452 non-null  Int64\n",
      " 3   floorNumber       17452 non-null  Int64\n",
      " 4   totalFloorNumber  17452 non-null  Int64\n",
      " 5   price             17452 non-null  Int64\n",
      "dtypes: Int64(6)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17452 entries, 65067453 to 77784815\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype\n",
      "---  ------            --------------  -----\n",
      " 0   carpetArea        17452 non-null  Int64\n",
      " 1   bedrooms          17452 non-null  Int64\n",
      " 2   bathrooms         17452 non-null  Int64\n",
      " 3   floorNumber       17452 non-null  Int64\n",
      " 4   totalFloorNumber  17452 non-null  Int64\n",
      " 5   price             17452 non-null  Int64\n",
      "dtypes: Int64(6)\n",
      "memory usage: 1.0 MB\n"
     ]
    }
   ],
   "source": [
    "df_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata['pricePerSqft'] = df_metadata['price'] / df_metadata['carpetArea']\n",
    "df_metadata['pricePerSqft'] = df_metadata['pricePerSqft'].round(0).astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('Data/cleaned_data.csv')\n",
    "temp['propertyId'] = temp['propertyId'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata['nameOfSociety'] = temp.set_index('propertyId').loc[df_metadata.index, 'nameOfSociety'].copy().astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "noImages = 0\n",
    "df_metadata['imagePaths'] = pd.NA\n",
    "for propertyId in df_metadata.index:\n",
    "    with open(f'Data/propertyDetails/{propertyId}.json', 'r', encoding='utf-8') as f:\n",
    "        propertyDetails = json.load(f)\n",
    "\n",
    "    if propertyDetails.get('propertyDetailInfoBeanData') is None:\n",
    "        noImages += 1\n",
    "        continue\n",
    "\n",
    "    temp = propertyDetails['propertyDetailInfoBeanData']['propertyDetail']['detailBean'].get('allImgPath')\n",
    "\n",
    "    if temp is None:\n",
    "        noImages += 1\n",
    "        continue\n",
    "\n",
    "    # imagePaths['imagePaths'][propertyId] = [path.removeprefix('https://img.staticmb.com') for path in temp]\n",
    "    df_metadata.at[propertyId, 'imagePaths'] = temp # [path.removeprefix('https://img.staticmb.com') for path in temp]\n",
    "\n",
    "noURLs = 0\n",
    "df_metadata['url'] = pd.NA\n",
    "for propertyId in df_metadata.index:\n",
    "    with open(f'Data/propertyDetails/{propertyId}.json', 'r', encoding='utf-8') as f:\n",
    "        propertyDetails = json.load(f)\n",
    "\n",
    "    if propertyDetails.get('propertyDetailInfoBeanData') is None:\n",
    "        noURLs += 1\n",
    "        continue\n",
    "\n",
    "    temp = propertyDetails['propertyDetailInfoBeanData']['propertyDetail']['detailBean'].get('url')\n",
    "\n",
    "    if temp is None:\n",
    "        noURLs += 1\n",
    "        continue\n",
    "\n",
    "    # imagePaths['imagePaths'][propertyId] = [path.removeprefix('https://img.staticmb.com') for path in temp]\n",
    "    df_metadata.at[propertyId, 'url'] = temp # [path.removeprefix('https://img.staticmb.com') for path in temp]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_2 = df_metadata.merge(df[categorical_features].copy(), how='outer', on='propertyId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_2['lastUpdatedDate'] = '7 Apr, 2025'\n",
    "\n",
    "df_metadata_2.to_pickle('PipelinesAndModels/RecommendationsPython/recommendations.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17452 entries, 11352855 to 78782315\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   carpetArea        17452 non-null  Int64   \n",
      " 1   bedrooms          17452 non-null  Int64   \n",
      " 2   bathrooms         17452 non-null  Int64   \n",
      " 3   floorNumber       17452 non-null  Int64   \n",
      " 4   totalFloorNumber  17452 non-null  Int64   \n",
      " 5   price             17452 non-null  Int64   \n",
      " 6   pricePerSqft      17452 non-null  Int64   \n",
      " 7   nameOfSociety     17452 non-null  object  \n",
      " 8   imagePaths        16474 non-null  object  \n",
      " 9   url               17452 non-null  object  \n",
      " 10  localityName      17452 non-null  category\n",
      " 11  transactionType   17452 non-null  category\n",
      " 12  furnished         17452 non-null  category\n",
      " 13  ageofcons         17452 non-null  category\n",
      " 14  lastUpdatedDate   17452 non-null  object  \n",
      "dtypes: Int64(7), category(4), object(4)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_metadata_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "request_body = {\n",
    "    'carpetArea': 800,\n",
    "    'bedrooms': 2,\n",
    "    'bathrooms': 2,\n",
    "    'floorNumber': 4,\n",
    "    'totalFloorNumber': 8,\n",
    "    'localityName': 'Kharadi',\n",
    "    'transactionType': 'New Property',\n",
    "    'furnished': 'Semi-Furnished',\n",
    "    'ageofcons': 'Under Construction'\n",
    "}\n",
    "\n",
    "response = requests.post(\"http://phpp-api.adityajoshi.in/predict\", json=request_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detail': 'Method Not Allowed'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking which properties are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhagy\\AppData\\Local\\Temp\\ipykernel_40176\\3462660137.py:4: DtypeWarning: Columns (10,11,30,31,32,34,35) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rawData = pd.read_csv(\"../Data/rawExtractedPropertyDetails.csv\")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "rawData = pd.read_csv(\"Data/rawExtractedPropertyDetails.csv\")\n",
    "rawData['propertyId'] = rawData['propertyId'].astype(str)\n",
    "rawData.set_index('propertyId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74208793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://www.magicbricks.com/propertyDetails/3-BHK-1662-Sq-ft-Multistorey-Apartment-FOR-Sale-Hinjewadi-in-Pune&id=4d423734323038373933'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "id = 0\n",
    "print(rawData.loc[df.index, ['url']].reset_index().iloc[id]['propertyId'])\n",
    "rawData.loc[df.index, ['url']].reset_index().iloc[id]['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['74208793', '73773015', '75162077', '76084143', '75658157', '48337347',\n",
       "       '74400799', '70265331', '75679247', '73281529', '73508741', '74837269',\n",
       "       '75692605', '75577751', '75665103', '74903023', '75933165', '73774411',\n",
       "       '75181979', '75746521', '75687865', '75691897', '72278837', '72991527',\n",
       "       '75657681', '75658553', '75198967', '74598267', '75709545', '75665171',\n",
       "       '74185523', '75206021', '76071257', '75694581', '76015789', '75688537',\n",
       "       '75147705', '74212763', '75658603', '73279171', '75382247', '73693493',\n",
       "       '75666349', '75668555', '63269545', '62164211', '75672063', '70861971',\n",
       "       '74918157', '74702487', '75668101', '71333765', '74717153', '75671737',\n",
       "       '74654451', '73625611', '75702115', '71242815', '74328807', '73877431',\n",
       "       '72510057', '75665359', '75617881', '75658549', '75378819', '71772115',\n",
       "       '75885301', '76141693', '75915281', '70762573', '71833131', '76172989',\n",
       "       '74729393', '76114639', '75664943', '75689699', '75685689', '75560657',\n",
       "       '74547651', '75674245', '64910947', '75679327', '75885977', '74438551',\n",
       "       '75663847', '75678135', '76149051', '73701963', '74233743', '71908491',\n",
       "       '72231675', '42857437', '75670809', '75667171', '75680031', '72255021',\n",
       "       '75692863', '75785999', '52729273', '75599071'],\n",
       "      dtype='string', name='propertyId')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.index[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_urls = rawData.loc[df.index[:100], 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Example: Assume property_urls is your pandas Series with propertyId as index and URL as values.\n",
    "# For example:\n",
    "# property_urls = pd.Series({\n",
    "#     101: \"https://example.com/property/101\",\n",
    "#     102: \"https://example.com/property/102\",\n",
    "#     ...\n",
    "# })\n",
    "\n",
    "# Set up Chrome options (you can add more options if needed)\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # run in headless mode if you don't need a GUI\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "chrome_options.page_load_strategy = \"eager\"\n",
    "\n",
    "# Initialize the webdriver (path to chromedriver may be needed)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Dictionary to store results:\n",
    "# True: property exists, False: property does not exist\n",
    "results = {}\n",
    "\n",
    "# Counter for requests to enforce sleep after every 100 requests\n",
    "request_count = 0\n",
    "\n",
    "for property_id, url in property_urls.items():\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Allow the page some time to load\n",
    "        # time.sleep(2)\n",
    "\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        # Check if the <body> tag has a class 'error'\n",
    "        body_element = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        body_class = body_element.get_attribute(\"class\")\n",
    "        \n",
    "        # Check for the specific error structure: body.error and a nested main tag with class \"content error\"\n",
    "        if \"error\" in body_class.split():\n",
    "            try:\n",
    "                # Try to locate the main tag with class \"content error\" inside body\n",
    "                driver.find_element(By.CSS_SELECTOR, \"main.content.error\")\n",
    "                # If found, mark property as not existing\n",
    "                results[property_id] = False\n",
    "            except NoSuchElementException:\n",
    "                # The structure doesn't match the error pattern; assume property is valid.\n",
    "                results[property_id] = True\n",
    "        else:\n",
    "            results[property_id] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions (e.g., network issues, selector issues, etc.)\n",
    "        print(f\"Error processing property {property_id}: {e}\")\n",
    "        results[property_id] = None  # or you could set it to False, or log the error\n",
    "\n",
    "    request_count += 1\n",
    "\n",
    "    # After every 100 requests, sleep for a minute to avoid overloading the server\n",
    "    if request_count % 1000 == 0:\n",
    "        print(f\"Processed {request_count} requests, sleeping for a minute...\")\n",
    "        time.sleep(15)\n",
    "\n",
    "# Close the driver when done\n",
    "driver.quit()\n",
    "\n",
    "# Optionally, convert the results dictionary to a pandas DataFrame or Series for further processing\n",
    "results_series = pd.Series(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     51\n",
       "False    49\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIENT SIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Preprocessor in ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # For saving JSON files\n",
    "import os   # For creating directories\n",
    "import skl2onnx # For ONNX conversion\n",
    "from skl2onnx.common.data_types import FloatTensorType, StringTensorType, Int64TensorType # For ONNX type definition\n",
    "# import numpy as np # You likely already have this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"PipelinesAndModels/RecommendationsOnnx/\" # Or your preferred directory name\n",
    "os.makedirs(output_dir, exist_ok=True) # Create the directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving preprocessor pipeline to ONNX...\n",
      "Preprocessor saved to: PipelinesAndModels/RecommendationsOnnx/recommendation_preprocessor.onnx\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Save Preprocessor as ONNX ---\n",
    "print(\"\\nSaving preprocessor pipeline to ONNX...\")\n",
    "onnx_filename = os.path.join(output_dir, \"recommendation_preprocessor.onnx\")\n",
    "\n",
    "# Define input types for ONNX conversion based on numerical features\n",
    "# IMPORTANT: Adjust this if you add categorical features back\n",
    "initial_types = [\n",
    "    # (\"localityName\", StringTensorType([None, 1])),\n",
    "    (\"carpetArea\", Int64TensorType([None, 1])), # Use Int64 if original is Int\n",
    "    (\"bedrooms\", Int64TensorType([None, 1])),\n",
    "    (\"bathrooms\", Int64TensorType([None, 1])),\n",
    "    (\"floorNumber\", Int64TensorType([None, 1])),\n",
    "    (\"totalFloorNumber\", Int64TensorType([None, 1])),\n",
    "    # (\"transactionType\", StringTensorType([None, 1])),\n",
    "    # (\"furnished\", StringTensorType([None, 1])),\n",
    "    # (\"ageofcons\", StringTensorType([None, 1])),\n",
    "]\n",
    "# Add types for categorical inputs here if they are used (e.g., StringTensorType)\n",
    "\n",
    "if not initial_types:\n",
    "    print(\"Error: No input features defined for ONNX conversion.\")\n",
    "else:\n",
    "    try:\n",
    "        # Ensure the preprocessor is fitted (it should be at this point)\n",
    "        if not hasattr(preprocessor, 'transformers_'):\n",
    "             print(\"Error: Preprocessor is not fitted. Cannot convert to ONNX.\")\n",
    "        else:\n",
    "             # Ensure input data used for fit_transform was float32, or cast here if needed\n",
    "            # X_encoded = preprocessor.fit_transform(X).astype(np.float32) # Make sure X_encoded is float32\n",
    "             # If X_encoded wasn't explicitly cast before, do it before this step or ensure StandardScaler outputs float32\n",
    "\n",
    "            onnx_model = skl2onnx.convert_sklearn(\n",
    "                preprocessor, # Your fitted preprocessor object\n",
    "                initial_types=initial_types,\n",
    "                target_opset={'':12, 'ai.onnx.ml': 3} # Adjust opset version if needed\n",
    "            )\n",
    "            with open(onnx_filename, \"wb\") as f:\n",
    "                f.write(onnx_model.SerializeToString())\n",
    "            print(f\"Preprocessor saved to: {onnx_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting preprocessor to ONNX: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving feature weights...\n",
      "Feature weights saved to: PipelinesAndModels/RecommendationsOnnx/recommendation_feature_weights.json\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Save Feature Weights ---\n",
    "print(\"\\nSaving feature weights...\")\n",
    "weights_filename = os.path.join(output_dir, \"recommendation_feature_weights.json\")\n",
    "try:\n",
    "    # feature_weights should be the dictionary you defined earlier\n",
    "    with open(weights_filename, 'w') as f:\n",
    "        json.dump(feature_weights, f, indent=2)\n",
    "    print(f\"Feature weights saved to: {weights_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving feature weights: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving final vectors with property IDs...\n",
      "Vectors saved to: PipelinesAndModels/RecommendationsOnnx/recommendations_property_vectors.json (17452 vectors)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Save Final Vectors with Property IDs ---\n",
    "print(\"\\nSaving final vectors with property IDs...\")\n",
    "vectors_filename = os.path.join(output_dir, \"recommendations_property_vectors.json\")\n",
    "\n",
    "vectors_data = []\n",
    "# final_encoded_vectors should be your DataFrame with weighted, ordered vectors\n",
    "for prop_id, vector_series in final_encoded_vectors.iterrows():\n",
    "    vector_list = vector_series.tolist() # Convert vector row to list\n",
    "    vectors_data.append({\n",
    "        \"id\": str(prop_id), # Ensure ID is string for JSON/RxDB\n",
    "        \"embedding\": vector_list\n",
    "    })\n",
    "\n",
    "try:\n",
    "    with open(vectors_filename, 'w') as f:\n",
    "        json.dump(vectors_data, f) # No indent for smaller file size\n",
    "    print(f\"Vectors saved to: {vectors_filename} ({len(vectors_data)} vectors)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving vectors: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving property metadata...\n",
      "Metadata saved to: PipelinesAndModels/RecommendationsOnnx/recommendations_property_metadata.json\n",
      "\n",
      "--- All components saved successfully! ---\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Save Property Metadata ---\n",
    "print(\"\\nSaving property metadata...\")\n",
    "metadata_filename = os.path.join(output_dir, \"recommendations_property_metadata.json\")\n",
    "\n",
    "try:\n",
    "    # df_metadata_2 should be your loaded and cleaned metadata DataFrame\n",
    "    # Create a copy to avoid modifying the original DataFrame if needed elsewhere\n",
    "    df_meta_copy = df_metadata_2.loc[final_encoded_vectors.index].copy()\n",
    "\n",
    "    # Prepare metadata for JSON: Convert problematic types\n",
    "    for col in df_meta_copy.select_dtypes(include=['Int64']).columns:\n",
    "         # Convert Int64 to float to handle potential NAs -> null in JSON\n",
    "         df_meta_copy[col] = df_meta_copy[col].astype(float)\n",
    "    for col in df_meta_copy.select_dtypes(include=['category']).columns:\n",
    "         df_meta_copy[col] = df_meta_copy[col].astype(str) # Convert category to string\n",
    "    # Add handling for other types like datetime if necessary\n",
    "\n",
    "    # Convert DataFrame to dictionary: { property_id: {col: value, ...} }\n",
    "    df_meta_copy.index = df_meta_copy.index.astype(str) # Ensure index is string key\n",
    "    metadata_dict = df_meta_copy.to_dict(orient='index')\n",
    "\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        # Use default=str as a fallback for any remaining non-serializable types\n",
    "        json.dump(metadata_dict, f, indent=0, default=str)\n",
    "    print(f\"Metadata saved to: {metadata_filename}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving metadata: {e}\")\n",
    "\n",
    "print(\"\\n--- All components saved successfully! ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
