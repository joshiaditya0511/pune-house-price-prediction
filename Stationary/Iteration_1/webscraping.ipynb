{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to extract query parameters from URL\n",
    "\n",
    "In Chrome, I opened the magicbricks site and searched for the properties with desired filters like city, budget, sorting by price, etc. Then, after inspecting the network tab in dev tools, I found the API they use to request property details in JSON form. I used the same parameters to make requests to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editSearch': 'Y',\n",
       " 'category': 'S',\n",
       " 'propertyType': '10002,10003,10021,10022',\n",
       " 'budgetMax': '45000000',\n",
       " 'city': '4378',\n",
       " 'page': '2',\n",
       " 'sortBy': 'Highest_Price',\n",
       " 'postedSince': '-1',\n",
       " 'pType': '10002,10003,10021,10022',\n",
       " 'isNRI': 'N',\n",
       " 'multiLang': 'en'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "# Original URL\n",
    "url = \"https://www.magicbricks.com/mbsrp/propertySearch.html?editSearch=Y&category=S&propertyType=10002,10003,10021,10022&budgetMax=45000000&city=4378&page=2&sortBy=Highest_Price&postedSince=-1&pType=10002,10003,10021,10022&isNRI=N&multiLang=en\"\n",
    "\n",
    "# Parse the URL\n",
    "parsed_url = urlparse(url)\n",
    "\n",
    "# Extract query parameters\n",
    "query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "# Clean up the values (optional: since parse_qs returns values as lists)\n",
    "query_params = {key: value[0] if len(value) == 1 else value for key, value in query_params.items()}\n",
    "\n",
    "query_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping resultLists to get property details in bulk\n",
    "\n",
    "After this data is scraped, the urls for each unique property are stored in a list. The list is then used to scrape the property details for each property separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename='scraping_errors.log',\n",
    "    filemode='a',\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Chrome options for headless browsing\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Runs Chrome in headless mode.\n",
    "chrome_options.add_argument('--no-sandbox')  # Bypass OS security model\n",
    "chrome_options.add_argument('--disable-gpu')  # Applicable to Windows OS only\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL and query parameters\n",
    "base_url = \"http://www.magicbricks.com/mbsrp/propertySearch.html\"\n",
    "\n",
    "query_params = {'editSearch': 'Y',\n",
    " 'category': 'S',\n",
    " 'propertyType': '10002,10003,10021,10022',\n",
    " 'budgetMax': '850000',\n",
    " 'city': '4378',\n",
    " 'page': '1',\n",
    " 'sortBy': 'Highest_Price',\n",
    " 'postedSince': '-1',\n",
    " 'pType': '10002,10003,10021,10022',\n",
    " 'isNRI': 'N',\n",
    " 'multiLang': 'en'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for saving JSONs\n",
    "os.makedirs('resultLists', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error tracking\n",
    "failed_attempts = 0\n",
    "MAX_FAILED_ATTEMPTS = 5\n",
    "\n",
    "# Main scraping loop\n",
    "page = 1\n",
    "counter = 1  # For JSON file naming\n",
    "\n",
    "# while True:\n",
    "for _ in range(100):\n",
    "    try:\n",
    "        # Update page number in query parameters\n",
    "        query_params['page'] = str(page)\n",
    "\n",
    "        # Construct the URL with query parameters\n",
    "        url_with_params = f\"{base_url}?\" + \"&\".join([f\"{k}={v}\" for k, v in query_params.items()])\n",
    "\n",
    "        # Navigate to the URL\n",
    "        driver.get(url_with_params)\n",
    "\n",
    "        # Wait for a moment to ensure the page has loaded\n",
    "        time.sleep(5)  # Adjust the sleep time if necessary\n",
    "\n",
    "        # Retrieve the JSON content using JavaScript execution\n",
    "        json_content = driver.execute_script(\"return document.body.innerText;\")\n",
    "\n",
    "        # Parse the JSON content\n",
    "        response_json = json.loads(json_content)\n",
    "\n",
    "        # Save JSON data\n",
    "        file_name = f\"Data/resultLists/list8LacMinus{page}.json\"\n",
    "        with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(response_json, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"Saved: {file_name}, with page={page}\")\n",
    "        counter += 1\n",
    "\n",
    "        # Reset failed attempts counter on success\n",
    "        failed_attempts = 0\n",
    "\n",
    "        # Increment page\n",
    "        page += 1\n",
    "\n",
    "        # Wait between requests\n",
    "        time.sleep(2)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        logging.error(f\"Failed for page={page} with error: {e}\")\n",
    "        failed_attempts += 1\n",
    "\n",
    "        # If too many failures, exit the loop\n",
    "        if failed_attempts >= MAX_FAILED_ATTEMPTS:\n",
    "            print(\"Too many consecutive failures. Stopping the scraping process.\")\n",
    "            break\n",
    "\n",
    "        # Continue to next iteration\n",
    "        continue\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to check for number of unique listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total JSON files processed: 2555\n",
      "Files with issues (not 30 listings or errors): [('list15CrPlus3.json', 2), ('list540LacPlus27.json', 13), ('list8LacMinus100.json', 0), ('list8LacMinus15.json', 6), ('list8LacMinus16.json', 0), ('list8LacMinus17.json', 0), ('list8LacMinus18.json', 0), ('list8LacMinus19.json', 0), ('list8LacMinus20.json', 0), ('list8LacMinus21.json', 0), ('list8LacMinus22.json', 0), ('list8LacMinus23.json', 0), ('list8LacMinus24.json', 0), ('list8LacMinus25.json', 0), ('list8LacMinus26.json', 0), ('list8LacMinus27.json', 0), ('list8LacMinus28.json', 0), ('list8LacMinus29.json', 0), ('list8LacMinus30.json', 0), ('list8LacMinus31.json', 0), ('list8LacMinus32.json', 0), ('list8LacMinus33.json', 0), ('list8LacMinus34.json', 0), ('list8LacMinus35.json', 0), ('list8LacMinus36.json', 0), ('list8LacMinus37.json', 0), ('list8LacMinus38.json', 0), ('list8LacMinus39.json', 0), ('list8LacMinus40.json', 0), ('list8LacMinus41.json', 0), ('list8LacMinus42.json', 0), ('list8LacMinus43.json', 0), ('list8LacMinus44.json', 0), ('list8LacMinus45.json', 0), ('list8LacMinus46.json', 0), ('list8LacMinus47.json', 0), ('list8LacMinus48.json', 0), ('list8LacMinus49.json', 0), ('list8LacMinus50.json', 0), ('list8LacMinus51.json', 0), ('list8LacMinus52.json', 0), ('list8LacMinus53.json', 0), ('list8LacMinus54.json', 0), ('list8LacMinus55.json', 0), ('list8LacMinus56.json', 0), ('list8LacMinus57.json', 0), ('list8LacMinus58.json', 0), ('list8LacMinus59.json', 0), ('list8LacMinus60.json', 0), ('list8LacMinus61.json', 0), ('list8LacMinus62.json', 0), ('list8LacMinus63.json', 0), ('list8LacMinus64.json', 0), ('list8LacMinus65.json', 0), ('list8LacMinus66.json', 0), ('list8LacMinus67.json', 0), ('list8LacMinus68.json', 0), ('list8LacMinus69.json', 0), ('list8LacMinus70.json', 0), ('list8LacMinus71.json', 0), ('list8LacMinus72.json', 0), ('list8LacMinus73.json', 0), ('list8LacMinus74.json', 0), ('list8LacMinus75.json', 0), ('list8LacMinus76.json', 0), ('list8LacMinus77.json', 0), ('list8LacMinus78.json', 0), ('list8LacMinus79.json', 0), ('list8LacMinus80.json', 0), ('list8LacMinus81.json', 0), ('list8LacMinus82.json', 0), ('list8LacMinus83.json', 0), ('list8LacMinus84.json', 0), ('list8LacMinus85.json', 0), ('list8LacMinus86.json', 0), ('list8LacMinus87.json', 0), ('list8LacMinus88.json', 0), ('list8LacMinus89.json', 0), ('list8LacMinus90.json', 0), ('list8LacMinus91.json', 0), ('list8LacMinus92.json', 0), ('list8LacMinus93.json', 0), ('list8LacMinus94.json', 0), ('list8LacMinus95.json', 0), ('list8LacMinus96.json', 0), ('list8LacMinus97.json', 0), ('list8LacMinus98.json', 0), ('list8LacMinus99.json', 0)]\n",
      "Total unique IDs collected: 33514\n",
      "Duplicate IDs found: 28306\n",
      "Total unique URLs collected: 33517\n",
      "Duplicate URLs found: 28303\n",
      "Duplicate IDs:\n",
      "Duplicate URLs:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Directory where JSON files are saved\n",
    "data_directory = '../Data/resultLists'\n",
    "\n",
    "# Initialize variables\n",
    "all_ids = []  # To collect all property IDs\n",
    "file_issues = []  # To track files with issues\n",
    "duplicate_ids = set()  # To track duplicate IDs\n",
    "\n",
    "all_urls = []\n",
    "duplicate_urls = set()\n",
    "\n",
    "# Process each JSON file\n",
    "for file_name in sorted(os.listdir(data_directory)):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(data_directory, file_name)\n",
    "\n",
    "        try:\n",
    "            # Read the JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "            \n",
    "            # Extract resultList\n",
    "            result_list = data.get('resultList', [])\n",
    "\n",
    "            # Check if resultList has exactly 30 items\n",
    "            if len(result_list) != 30:\n",
    "                file_issues.append((file_name, len(result_list)))\n",
    "\n",
    "            # Extract IDs and check for duplicates\n",
    "            ids_in_file = [property_data['id'] for property_data in result_list if property_data.get('price') is not None]\n",
    "            for property_id in ids_in_file:\n",
    "                if property_id in all_ids:\n",
    "                    duplicate_ids.add(property_id)\n",
    "                else:\n",
    "                    all_ids.append(property_id)\n",
    "\n",
    "            # Extract IDs and check for duplicates\n",
    "            urls_in_file = [property_data['url'] for property_data in result_list if property_data.get('price') is not None]\n",
    "            for property_url in urls_in_file:\n",
    "                if property_url in all_urls:\n",
    "                    duplicate_urls.add(property_url)\n",
    "                else:\n",
    "                    all_urls.append(property_url)\n",
    "        \n",
    "        except Exception as e:\n",
    "            file_issues.append((file_name, f\"Error reading file: {e}\"))\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_ids_list = list(duplicate_ids)\n",
    "duplicate_urls_list = list(duplicate_urls)\n",
    "\n",
    "# Report results\n",
    "print(f\"Total JSON files processed: {len(os.listdir(data_directory))}\")\n",
    "print(f\"Files with issues (not 30 listings or errors): {file_issues}\")\n",
    "print(f\"Total unique IDs collected: {len(all_ids)}\")\n",
    "print(f\"Duplicate IDs found: {len(duplicate_ids_list)}\")\n",
    "\n",
    "print(f\"Total unique URLs collected: {len(all_urls)}\")\n",
    "print(f\"Duplicate URLs found: {len(duplicate_urls_list)}\")\n",
    "\n",
    "if duplicate_ids_list:\n",
    "    print(\"Duplicate IDs:\")\n",
    "    # print(duplicate_ids_list)\n",
    "else:\n",
    "    print(\"No duplicate IDs found.\")\n",
    "\n",
    "if duplicate_urls_list:\n",
    "    print(\"Duplicate URLs:\")\n",
    "    # print(duplicate_urls_list)\n",
    "else:    \n",
    "    print(\"No duplicate URLs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving unique listings separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Unique JSONs saved to scraped_data2. Combined JSON saved to combined.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Directories\n",
    "input_directory = '../Data/resultLists'  # Replace with your input directory path\n",
    "unique_json_directory = '../Data/uniqueResults'  # Directory to save unique JSONs\n",
    "combined_json_file = '../Data/uniqueResultsCombined.json'  # Path for combined JSON\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(unique_json_directory, exist_ok=True)\n",
    "\n",
    "# Dictionary to store unique properties by their ID\n",
    "unique_properties = {}\n",
    "\n",
    "# Process each JSON file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            try:\n",
    "                data = json.load(file)\n",
    "                result_list = data.get('resultList', [])\n",
    "                for property in result_list:\n",
    "                    property_id = property.get('id')\n",
    "                    if property_id and property_id not in unique_properties:\n",
    "                        # Add to unique properties\n",
    "                        unique_properties[property_id] = property\n",
    "\n",
    "                        # Save this property as a separate JSON file\n",
    "                        unique_file_path = os.path.join(unique_json_directory, f\"{property_id}.json\")\n",
    "                        with open(unique_file_path, 'w', encoding='utf-8') as unique_file:\n",
    "                            json.dump(property, unique_file, indent=4)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON in file {filename}: {e}\")\n",
    "\n",
    "# Save the combined JSON\n",
    "with open(combined_json_file, 'w', encoding='utf-8') as combined_file:\n",
    "    json.dump(unique_properties, combined_file, indent=4)\n",
    "\n",
    "print(f\"Processing complete. Unique JSONs saved to {unique_json_directory}. Combined JSON saved to {combined_json_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting list of property ids and urls for further scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file with id and url has been created at properties.csv.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the combined JSON file and output CSV file\n",
    "combined_json_file = '../Data/uniqueResultsCombined.json'  # Replace with the actual path\n",
    "csv_output_file = '../Data/idsAndUrls.csv'  # Replace with the desired CSV file path\n",
    "\n",
    "# Load the combined JSON file\n",
    "with open(combined_json_file, 'r', encoding='utf-8') as file:\n",
    "    combined_data = json.load(file)\n",
    "\n",
    "# Prepare data for the CSV\n",
    "csv_data = [{'id': prop_id, 'url': prop_data.get('url')} for prop_id, prop_data in combined_data.items()]\n",
    "\n",
    "# Create a DataFrame and save it as a CSV\n",
    "df = pd.DataFrame(csv_data)\n",
    "df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "print(f\"CSV file with id and url has been created at {csv_output_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping details from each property's webpage\n",
    "\n",
    "The data obtained from the resultLists scraping is incomplete. However, it contains url for each property. We can use the url to scrape the details of each property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base variables\n",
    "base_url = \"https://www.magicbricks.com/propertyDetails/\"\n",
    "output_directory = \"../Data/propertyDetails\"\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv(\"../Data/idsAndUrls.csv\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--headless\")  # Run in headless mode\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "with open('error_ids.txt', 'r') as f:\n",
    "    error_ids = f.read().splitlines()\n",
    "\n",
    "try:\n",
    "    for i in range(29999, 33889):\n",
    "        try:\n",
    "            # Get property details\n",
    "            property_id = df.iloc[i]['id']\n",
    "            property_url = df.iloc[i]['url']\n",
    "            full_url = f\"{base_url}{property_url}\"\n",
    "\n",
    "            # Navigate to the URL\n",
    "            driver.get(full_url)\n",
    "\n",
    "            # Wait for the JavaScript variable to load\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            property_details = wait.until(\n",
    "                lambda d: d.execute_script(\"return window.SERVER_PRELOADED_STATE_DETAILS\")\n",
    "            )\n",
    "\n",
    "            # Save the JSON data to a file\n",
    "            output_file = os.path.join(output_directory, f\"{property_id}.json\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(property_details, f, indent=4)\n",
    "\n",
    "            print(f\"Successfully saved data for property ID: {property_id} with iter number: {i}\")\n",
    "\n",
    "            # time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            error_ids.append(property_id)\n",
    "            print(f\"Error processing iteration: {i} with property ID {property_id}: {e}\\nURL: {full_url}\\n\\n\")\n",
    "\n",
    "finally:\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "    print(f\"Total Error Ids: {len(error_ids)}\\nError Ids: {error_ids}\")\n",
    "    with open('error_ids.txt', 'a') as f:\n",
    "        error_ids = [str(i) for i in error_ids]\n",
    "        f.write('\\n'.join(error_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Scraping re-run for error_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Base variables\n",
    "base_url = \"https://www.magicbricks.com/propertyDetails/\"\n",
    "output_directory = \"Data/propertyDetails\"\n",
    "\n",
    "# Load DataFrame\n",
    "df = pd.read_csv(\"../Data/idsAndUrls.csv\", index_col='id')\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "with open('error_ids.txt', 'r') as f:\n",
    "    error_ids = f.read().splitlines()\n",
    "\n",
    "error_ids = [int(i) for i in error_ids]\n",
    "error_ids = list(set(error_ids))\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, property_id in enumerate(error_ids):\n",
    "        try:\n",
    "            # Get property details\n",
    "            property_url = df.loc[property_id]['url']\n",
    "            full_url = f\"{base_url}{property_url}\"\n",
    "\n",
    "            # Navigate to the URL\n",
    "            driver.get(full_url)\n",
    "\n",
    "            # Parse the page source with BeautifulSoup\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Check if the body tag has class \"error\"\n",
    "            body_tag = soup.find('body')\n",
    "            if body_tag and 'error' in body_tag.get('class', []):\n",
    "                print(f\"Error page detected for id: {property_id} and iter number: {i}\")\n",
    "                continue  # Skip further processing for this URL\n",
    "\n",
    "            # Wait for the JavaScript variable to load\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            property_details = wait.until(\n",
    "                lambda d: d.execute_script(\"return window.SERVER_PRELOADED_STATE_DETAILS\")\n",
    "            )\n",
    "\n",
    "            # Save the JSON data to a file\n",
    "            output_file = os.path.join(output_directory, f\"{property_id}.json\")\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(property_details, f, indent=4)\n",
    "\n",
    "            print(f\"\\nSuccessfully saved data for property ID: {property_id} with iter number: {i}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            error_ids.append(property_id) if property_id not in error_ids else None\n",
    "            print(f\"Error processing iteration: {i} with property ID {property_id}: {e}\\nURL: {full_url}\\n\\n\")\n",
    "\n",
    "finally:\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "    print(f\"Total Error Ids: {len(error_ids)}\\nError Ids: {error_ids}\")\n",
    "    with open('error_ids.txt', 'w') as f:\n",
    "        error_ids = [str(i) for i in error_ids]\n",
    "        f.write('\\n'.join(error_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points saved to Data\\MiscellaneousResources\\OpenStreetMapsData\\Pune_Landmarks_and_Localities_geojson\\LandmarkPoints.json\n",
      "Polygons saved to Data\\MiscellaneousResources\\OpenStreetMapsData\\Pune_Landmarks_and_Localities_geojson\\LandmarkPolygons.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the GeoJSON file\n",
    "input_file = r\"Data\\MiscellaneousResources\\OpenStreetMapsData\\Pune_Landmarks_and_Localities_geojson\\Pune_Landmarks_and_Localities.geojson\"  # Replace with your file name\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    geojson_data = json.load(file)\n",
    "\n",
    "# Initialize separate lists for points and polygons\n",
    "points = []\n",
    "polygons = []\n",
    "\n",
    "# Iterate through the GeoJSON features\n",
    "for feature in geojson_data['features']:\n",
    "    geometry_type = feature['geometry']['type']\n",
    "    if geometry_type == \"Point\":\n",
    "        points.append(feature)\n",
    "    elif geometry_type in [\"Polygon\", \"MultiPolygon\"]:\n",
    "        polygons.append(feature)\n",
    "\n",
    "# Create GeoJSON structures for points and polygons\n",
    "points_geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": points\n",
    "}\n",
    "\n",
    "polygons_geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": polygons\n",
    "}\n",
    "\n",
    "# Save the points and polygons GeoJSON to separate files\n",
    "points_output_file = r\"Data\\MiscellaneousResources\\OpenStreetMapsData\\Pune_Landmarks_and_Localities_geojson\\LandmarkPoints.json\"\n",
    "polygons_output_file = r\"Data\\MiscellaneousResources\\OpenStreetMapsData\\Pune_Landmarks_and_Localities_geojson\\LandmarkPolygons.json\"\n",
    "\n",
    "with open(points_output_file, 'w') as file:\n",
    "    json.dump(points_geojson, file, indent=4)\n",
    "\n",
    "with open(polygons_output_file, 'w') as file:\n",
    "    json.dump(polygons_geojson, file, indent=4)\n",
    "\n",
    "print(f\"Points saved to {points_output_file}\")\n",
    "print(f\"Polygons saved to {polygons_output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
